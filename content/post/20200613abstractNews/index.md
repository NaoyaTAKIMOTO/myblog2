---
title: '短文ニュースを作るための要約文生成AIをつくったった'
date: 2020-06-13T00:24:00.004+09:00
draft: false
aliases: [ "/2020/06/ai.html" ]
tags : [gpu,技術系,文短縮,技術,文生成,深層学習,自然言語処理]
---

深層学習を用いた機械翻訳モデルを用いて、ニュース本文からタイトルを自動生成するモデルの学習に成功しました。
## 追記
- 2022時点なら[T5を使ってファインチューニングを試す]({{<ref "/post/20210701T5/index.md">}})方が簡単な気がする。


## 前振り

以前にネットニュース向けに、原稿からタイトルを自動で作れないか？という案件に携わることがありました。

取り組むにあたり、既存の手法について調べてました。

すると朝日新聞がすでに似たような技術に着手していることが分かりました。

[電車内の短文ニュース向け要約を自動化、９０分が１分に](https://prtimes.jp/main/html/rd/p/000000809.000009214.html)

まあ、朝日新聞は有料で文章要約APIを公開しています。

朝日新聞のお陰で、タイトル生成は技術的に実現可能なことが分かりました。 そこで自分でも自動タイトル生成モデルを作ってみることにしました。

技術的な詳細については、流石に商売の種なのか、 関連するリンクを漁ってみても有用な情報は見当たりませんでした。

同じようなニュースっぽい記事が見つかるばかりである。

そこで論文ベースでの調査に切り替えたところ、参考になりそうな情報が手に入りました。

## 論文調査

ニュース記事から関連リンクを調べても技術的な詳細には踏み込めませんでした。

そこで調査の範囲を論文にまで広げることにしました。

実用のレベルに達している技術であれば、 当然のことながら研究段階のレベルはとうの昔に過ぎている筈です。

英語論文まで調査範囲を広げれば、実装のために使える情報が得られるでしょう。

そして研究論文は、 特に情報技術系統はGoogleScholarで検索すればPDFが容易にヒットする時代です。

個人的にはGoogleScholarよりも[semantic scholar](https://www.semanticscholar.org/)の方が関連ワードや引用元へのアクセスが容易いのでこちらを主に利用してます。

自分が詳しくない分野について有効な検索ワードの絞り込み自体が一つの難所なのですが、semantic scholar のおかげでそのハードルを下げることができました。

適当に"自然言語処理+要約"あたりでサクサクと検索したところ、日本語情報では

[複数エンコーダを用いたヤフートピックス見出し候補生成](https://research-lab.yahoo.co.jp/nlp/20180326_kobayashi.html)

[朝日新聞社メディアラボの人工知能研究の取り組み](https://www.jstage.jst.go.jp/article/jkg/68/12/68_591/_pdf/-char/ja)

などが見つかりました。

要点としては、言語モデルに機械翻訳をさせる要領で、 ニュース本文とタイトルの対を学習させるというものです。

モデルはニュース本文からタイトルを生成するように学習をします。

さらに文献調査を進めたところ、 研究の方向性はタイトルの生成から、 そこに文字数の制約を設けるところまで至っていることがわかりました。

これはかなり実応用を意識した問題設定のようです。

文字数の制約という問題設定は表意文字の文化圏で強く顕在化しており、 英語論文は見つけられませんでした。

そうとなれば後は、 教師データを集めて、モデルを記述し、計算機にかけるだけです。

## 実験

### データ

教師データはニュースサイトをスクレイピングし、約二万対のニュース本文とタイトルを用意しました。

### モデル

モデルはAlbertを参考に多段transformerを用いました。

encoderーdecoderモデルです。

AlbertはBERTからパラメータ数を効率化したモデルです。同じtransformerの層を繰り返し経由することで、モデルサイズの削減を行っています。 詳しくは[BERTはまだまだ進化する！軽くて強いALBERTが登場！](https://ai-scholar.tech/articles/treatise/albert-ai-227)を参照してください。

またencoderーdecoderモデルを採用した理由としては、ようやくモデルであるPEGASUS、BARTなどが同様の構造を採用しているからです。

今回のモデルは深層学習系のattenntionモデルのため、 学習にはGPUやTPUを必要となります。

元論文と同様にRNN系のモデルを用いてもいいのですが、 最近の流行はtransformer系統なのでこちらを用いました。

個人的にもtranformerモデルの方が扱いに親しみが持てます。

文献をいくつか読んだ限りでは計算効率や性能面もtransformer系統の方がよさそうでした。

### 開発環境

環境の良さからpython-pytorchを用いた。

すでに深層学習モデルの基本的なコンポーネントや道具が整備されており、車輪の再発明をせずに済みます。また記述が楽なのでプロトタイプに向いています。

### 結果

GPU 11GB✖️2✖️5days で途中経過を観察しました。

おそらく数時間程度で出力結果は収束していました。

この問題の評価の難しいところとして、汎化性能の評価方法が挙げられます。

文章の自然さを評価する方法については、2020年時点ではクラウドソーシングを使った人力での採点が最も人間の感性に近いとされており、自動採点はホットなトピックです。

そのため、今回の結果の評価には私自身の目視での確認を採用しました。

結果としては、それなりに見られるものを出力するところまでは至りました。

## 感想

生成文の中にいくつかの場所で不自然なものに置き換わっていたり、人間からすると不思議な間違い方をします。

しかし、出力結果と本文を読めば、後は素人でも多少の修正を加えるだけでも実用に耐えるでしょう。

## 考察


トークンごとに意味ベクトルを学習させると、さらに結果が安定しました。

企業名や固有名詞に誤りが含まれるのは、モデルの損失関数からすると些細な間違いだが、人間からすると大きな間違いとして認識します。

このような間違いかたは頻発していました。

この点においては教師データの量を増やすか、コピーメカニズムをモデルに組み込むかのどちらかで対処するのが良いでしょう。

入力文中の価格や年度といった数字に対しては非常に脆弱であることが分かりました。

数字に該当する部分には数字が出力されますが、異なる数字であることが殆どでした。

それは数字の意味を把握するための機構がモデルに備わっていないので仕方がないといえます。

質疑応答システムに関する研究では、 transformer系統の機構とは別に、 数字を扱うための別の機構を加えて一つの大きなシステムを組み上げる手法がリーダーボードの上位を占めていたので、この弱点は既知のものです。

しかし、まだスマートな解決方法が見つかっていないのが正直なところです。

[AI2 Leaderboard](https://leaderboard.allenai.org/drop/submissions/public)

## まとめ
-----------------

文章を書くのも、要約するのも同じようなものとして深層学習の文脈では扱える。

そして深層学習に共通する課題として教師データと学習資源の確保は必須だ。

しかし、その点を解決できればそれなり以上の結果も得られる。

<!-- MAF Rakuten Widget FROM HERE -->
<script type="text/javascript">MafRakutenWidgetParam=function() { return{ size:'468x160',design:'slide',recommend:'on',auto_mode:'on',a_id:'2220301', border:'off'};};</script><script type="text/javascript" src="//image.moshimo.com/static/publish/af/rakuten/widget.js"></script>
<!-- MAF Rakuten Widget TO HERE -->